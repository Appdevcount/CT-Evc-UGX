Parameters	Git-flow	Github Lab(Extention of Trunk based Development)
Summary of Usecase	Gitflow is a branching strategy designed for projects with a more structured and formal release process. It emphasizes the separation of development work from release preparation and maintenance	GitHub Flow is a simpler, more lightweight branching strategy that focuses on continuous delivery and frequent releases. It is optimized for projects where the release process is less formal.
Branches	Main Branches:
main (or master): Represents the production-ready code.
develop: Serves as the main integration branch for ongoing development.
Feature Branches: Created for new features or enhancements and branched from develop.
Release Branches: Created when preparing for a release and branched from develop.
Hotfix Branches: Used for critical bug fixes in the production code and branched from main.	Main Branch:
main (or master): Represents the production-ready code.
Feature Branches: Created for new features or bug fixes and branched from main.
Workflow	-Features are developed in isolation in feature branches and merged back into develop.
-Release branches are created when ready for stabilization, and no new Features are added to them.
-Hotfix branches are used for urgent production fixes.	-Features are developed in isolated feature branches, where code is regularly committed and pushed.
-Pull Requests (PRs) are used to propose and review changes. Collaboration and code review happen within the PR.
-Once a PR is approved and passes automated tests, it can be merged directly into main.
Release Management	-Emphasizes a structured release process with dedicated release branches.
-Well-suited for software projects with scheduled releases and the need for release candidates.	-GitHub Flow doesn't have dedicated release branches. Releases are made by tagging specific commits in main.
-Suitable for projects that continuously deploy changes and where releases can happen frequently.
Complexity	Gitflow is more structured and has more branches, which can make it seem complex	GitHub Flow is simpler and easier to adopt. Simpler and easier to manage, especially in small to medium-sized teams.
On-Demand Release vs. Scheduled Release	Supports scheduled releases by design, with the release branch acting as a stabilization phase	Typically favors on-demand releases due to continuous integration practices.
Number of Scrum Teams	Can become complex with multiple teams, as merging feature branches into the release branch may require synchronization efforts	Scales well with multiple teams due to continuous integration, allowing parallel development and faster integration
Microservices	Can be adapted for microservices but may introduce coordination challenges during the release phase.	Suitable for microservices architecture, as it encourages frequent integration and deployment of smaller code changes
Release Frequency	Typically leads to less frequent but larger releases, which may be better for certain types of software	Supports frequent and smaller releases due to its continuous integration nature.
Code Stability	Provides a dedicated stabilization phase (release branch) for rigorous testing and bug-fixing.	Codebase tends to be more stable since integration is continuous, but extensive automated testing is crucial
Feature Isolation	Features are isolated in feature branches until they are merged into the release branch.	Encourages feature flags and toggles to isolate unfinished features until they are ready for release
Hotfixes and Urgent Releases	May require a separate hotfix process, as urgent changes can be complex to integrate into the release branch	Allows for immediate hotfixes and urgent releases, as changes are continuously integrated.
Visibility	Provides a clear separation between development and release phases, enhancing visibility into the release process.	Offers better visibility into the status of features and changes as they progress through development.
Risk Management	Mitigates risk through dedicated stabilization phases, but may have longer release cycles.	Lower risk due to continuous integration, but requires robust testing and monitoring.



Thanks and Regards
Siraj

From: R, Sirajudeen (CTR) 
Sent: Thursday, August 22, 2024 12:01 PM
To: R, Sirajudeen (CTR) <sirajudeen.r@evicore.com>
Subject: RE: dsgns prcs

Qlik replicate

Thanks and Regards
Siraj

From: R, Sirajudeen (CTR) 
Sent: Thursday, August 22, 2024 11:56 AM
To: R, Sirajudeen (CTR) <sirajudeen.r@evicore.com>
Subject: RE: dsgns prcs

UCX Platform
•	 
•	 
•	 
•	 
Owned by Rajkumar Subaschandra bose
Last updated: May 01, 2024 by Kayla Howell (Unlicensed)
1 min read
127 people viewed Attachments
UCX is an EverNorth internal portal for clinicians, Nurses and MDs, who perform clinical reviews on UM requests. UCX currently supports Sleep, Gastro, DME, Radiology & Cardiology medical disciplines from both eP and ImageOne platforms for MDs. Next major milestone is to get Nurses enabled for these medical disciplines. The ultimate goal is to get all programs and users migrated from CDP (IO platform) and eP clinical (eP platform) to UCX and retire CDP and eP Clinical.
UCX will eventually be available for enterprise needs as part of Clinical Care Management (CCM), our goal is to enable the UCX for Cigna Behavioral Health, Cigna Government and Cigna Commercial programs. UCX will be integrated with the TruCare or MHK products especially for Clinical Review.

Logging Optimization and other cost saving recommendations.
•	
•	
Owned by David Ovitz
Last updated: Dec 05, 2023 by Vilesh Malhotra
3 min read
31 people viewed Attachments
Log level recommendations.
•	We are recommending using configuration to control the logging level, so that we can have different level across environments and also an option to update them as and when required.
•	The log level that we are recommending is “Warning” for DEV and INTG as default, can be change temporarily as the need arises.
•	To get the most (cost saving) out of this activity we also recommend a higher log level in PROD, So, if you feel the domain/app is in a mature enough state the log level could be Warning, if not then use your discretion to set a lower log level like Information or Debug.
Common library updates.
•	ucx.gravity.common to >= 4.1.4
This version has updated code to have logs generated with the JSON from kafka in them, neither during production nor consumption. So, if you have any apps using this lib, upgrade to the desired version.
•	evicore.eventsource.cosmos to >= 1.0.4
This version internally uses ucx.gravity.common. So, if you have any apps using this lib, upgrade to the desired version.
•	evicore.ucx.patterns to >= 1.2.0
This version of the library was updated to support configuration when setting up Serilog alongside a standard Console and AppInsights sink. So, if you have any apps using this lib, upgrade to the desired version. One way to know if you are using log extensions from this lib is if you see this in your app.
var builder = WebApplication.CreateBuilder(args);
builder.AddSerilogAndAppInsights("AppRoleName", "Platform", "AppInsights-InstrumentationKey");
Application related updates.
Other than updating the common libraries, you may need to do some additional working to optimize log for the app.
•	If your app is also generating logs with the JSON of object with PHI in it, like kafka message, update them to not log them, as the libs will only take care of the common code it contains.
•	Alongside updating evicore.ucx.patterns, you will have to add the supporting configuration in your app.
Following is an example configuration for configuring logLevel in Serilog or Microsoft logging extensions.
{
"Logging": {
"LogLevel": {
"Default": "Debug",
"Microsoft.AspNetCore": "Warning",
"Microsoft": "Warning",
"Microsoft.Hosting.Lifetime": "Warning"
}
},
"Serilog": {
"Using": [],
"MinimumLevel": {
"Default": "Warning"
}
}
}
Configuration under "Serilog" is for logs generated using the Serilog library, the one above it "Logging" is for log generated from other things using Microsoft extensions, e.g. health-check etc.
•	If the app is not using evicore.ucx.patterns but still using serilog, make sure to read from configuration.
•	 var config = ConfigurationHelper.GetConfiguration();
•	 var loggerConfiguration = new LoggerConfiguration().ReadFrom.Configuration(config);
•	 var telemetryConfiguration = TelemetryConfiguration.CreateDefault();
•	 telemetryConfiguration.InstrumentationKey = config.GetValue<string>("ApplicationInsights:InstrumentationKey");
 loggerConfiguration.WriteTo.ApplicationInsights(telemetryConfiguration, TelemetryConverter.Traces);
Code and Configuration can be used together as long as they aren’t controlling the same thing.
•	Use and environment variable to control the log level config in terraform.
"Logging__LogLevel__Default" = var.log_level_api
"Serilog__MinimumLevel__Default" = var.log_level_api
Validations
Query the respective AppInsights/Log analytics resouce use by the domain/App to validate if no large JSONs are logged along with the serverityLevel match to the log level set in the configuration.
sample queries for Log Analytics and AppInsights respectively.
AppTraces
| project AppRoleName, Message, SeverityLevel, TimeGenerated
| where AppRoleName has 'Ucx.AclService.Erss.Observer'
traces
| project cloud_RoleName, message, severityLevel, timestamp
| where cloud_RoleName has "Attachments"

Add label


API Service Auto Scaler
•	 
Owned by David Ovitz
Last updated: Dec 01, 2023
1 min read
10 people viewed Attachments
This example is from UCX.DiagnosisService. This only should be applied to the API Services.
https://dev.azure.com/eviCoreDev/UCX/_git/Ucx.Diagnosis?path=/Ucx.Diagnosis.Service/Infrastructure/app-service-api-scaler.tf&version=GBmain&_a=contents
The minimum and maximum scale would be per environment, just remember you have to have enough available IPs on your private endpoints to be able to scale all the way up to maximum.

Add label

resource "azurerm_monitor_autoscale_setting" "api_scaler" {
  name                = "ucx-diagnosis-api-auto-scale-cpu"
  resource_group_name = var.resource_group_name
  location            = var.location
  target_resource_id  = module.service_plan_api.id

  profile {
    name = "cpu scaler"

    capacity {
      default = 1
      minimum = var.api_auto_scale_minimum
      maximum = var.api_auto_scale_maximum
    }

    rule {
      metric_trigger {
        metric_name        = "CPUPercentage"
        metric_resource_id = module.service_plan_api.id
        time_grain         = "PT1M"
        statistic          = "Average"
        time_window        = "PT5M"
        time_aggregation   = "Average"
        operator           = "GreaterThan"
        threshold          = 75
        metric_namespace   = "Microsoft.Web/serverfarms"
      }

      scale_action {
        direction = "Increase"
        type      = "ChangeCount"
        value     = "1"
        cooldown  = "PT15M"
      }
    }

    rule {
      metric_trigger {
        metric_name        = "CPUPercentage"
        metric_resource_id = module.service_plan_api.id
        time_grain         = "PT1M"
        statistic          = "Average"
        time_window        = "PT5M"
        time_aggregation   = "Average"
        operator           = "LessThan"
        threshold          = 33
      }

      scale_action {
        direction = "Decrease"
        type      = "ChangeCount"
        value     = "1"
        cooldown  = "PT5M"
      }
    }
  }

  tags = {
    Tier               = local.tags.Tier
    CostCenter         = local.tags.CostCenter
    ITSponsor          = local.tags.ITSponsor
    AssetOwner         = local.tags.AssetOwner
    AppName            = local.tags.AppName
    BusinessOwner      = local.tags.BusinessOwner
    DataClassification = local.tags.DataClassification
    ServiceNowBA       = local.tags.ServiceNowBA
    ServiceNowAS       = local.tags.ServiceNowAS
    SecurityReviewID   = local.tags.SecurityReviewID
  }
}


RU Settings by container
•	 
•	 
•	 
•	+2
Owned by David Ovitz
Last updated: Apr 05, 2024 by Colin Gilbert
1 min read
34 people viewed Attachments
The RUs on containers are moving down to reduce costs based on an analysis of a week’s worth of data at a granularity of 1 minute. Then adding 30% to the highest peak, to safeguard us from overdoing it.
What does this mean for you? This means that when you want to do a heavy load operation on a container like a replay, or a rehydration, you’ll need to scale it up to a higher value. Then after it’s done, scale it back down to the calculated value.
This chart will have what we’re now at, and what it was at before we modified them. A couple did actually have to go up, but most went down quite far.
 
Account: *cdbucxaclservice
DB: ACLServiceDB
container	environment	new value	old value
AuditLog	dv1	2000	10000

UCX - P2P Diagrams
•	 
Owned by Vilesh Malhotra
Last updated: May 29, 2024
2 min read
40 people viewed Attachments
Business use case
Clinicians (MDs and Nurse) need a way to search/retrieve requests created in ImageOne and eviCore Platform (possible other source systems in future) and then perform reviews by using the request’s details provided by the application.
Logical Diagram
 
UCX has multiple services which perform different roles, like searching a request, getting the next request to be reviewed, getting various state for a request etc. For each such role a separate resource group is created within eu2-ucx-test subscription with the following pattern stage_eu2_ucx_(service). 

Private endpoints
All our databases/keyvaults/SignalR services have restricted public access, the application code is hosted via App Services within a private subnet (per Resource group) and connects to the said resources via private endpoints.
Integration VNETs
All Application Services within the UCX subscription will be under the stage-ucx-eu2-vnet, and communication outside the VNET will be assisted via integration VNETs, to either connect with our enterprise cloud Kafka or expose our APIs via APIM.

Application Gateway
For uses cases of exposing endpoint outside the VNET, where we aren’t using APIM which has its own application gateway, e.g. SignalR hub will be exposed by our Application Gateway in our common resource group. 
Azure Front Door
A single Azure Front Door resource will be used to expose all micro frontends or static websites. The managed WAF policy should be enabled on all routes.
 
Alongside that we have a common resource group stage_eu2_ucx which will house all the shared resources. Refer resource group diagram for more details. 
Cosmos Rehydration - UCX Services
•	 
Owned by Gopal Kamdi
Dec 08, 2023
3 min read
34 people viewed Attachments
Rehydration is the process of re-processing the events when we have event sourcing (cosmos or any source) when we want to resend the message/event for some purpose. it's like replay in kafka, but as we have middle-ware event sourcing then instead of going back to previous step to kafka replay we can re-generate the state by collecting all messages/events related to a single or all request and sort them in sequence according to their sent date, create the state out of it and then send the further events as needed.
When we need rehydration?
When we want to bump the event version.
When we are correcting any mapping in event fields (if earlier incorrect mapping)
When we are adding new fields in the event, then for historical events when we want to populate those fields.
When cannot we do rehydration and need to go for kafka replay?
Whenever we do not have new fields available in event source messages then we cannot get that data by rehydration. In this case we need to replay kafka topic which will further rehydrate the event source as well.
Pre-requisites:
Prepare/all is the #1 end point we need to hit to start the full rehydration process, before we kicked off the end point we need to adjust the RU’s on all containers which are going to hit in this process for any update or insert bulk of records in short time span.
ACLService.Erss
Introduction	The ACLService.Erss Service primarily consumes the Kafka topic XPLErssFinal and XPLErssCaseEnrichedIone to produce platform agnostic events for UCX. Also, it acts as an ACL layer for UCX.
Getting Started	There are two applications in this git repository, Ucx.AclService.Erss.Api and Ucx.AclService.Erss.App.
There is a Properties/launchSettings.json for both projects that have local run/debug configurations.
Event Source	The data for this service is event sourced.
It uses evicore.eventsource and evicore.eventsource.cosmos.

Rehydration End Points 	Dev: Swagger UI (eheu2dv1-as-ucxaclserviceerss-api.azurewebsites.net)
INTG: Swagger UI (eheu2in1-as-ucxaclserviceerss-api.azurewebsites.net)
PROD: Swagger UI (ehcuspd1-as-ucxaclserviceerss-api.azurewebsites.net)

Methods	1.	Prepare all
o	This has to complete before #2 should be kicked off.
o	This is a background task and the endpoint will return an immediate 202 Accepted.

POST request DEV
URL- https://eheu2dv1-as-ucxaclserviceerss-api.azurewebsites.net/api/eventsource/rehydrate/RequestState/prepare/all
Content-Type: application/json
Settings - {"throttleMs":5,"takePerThrottle":5}
POST request INTG
URL- https://eheu2in1-as-ucxaclserviceerss-api.azurewebsites.net/api/eventsource/rehydrate/RequestState/prepare/all
Content-Type: application/json
Settings - {"throttleMs":5,"takePerThrottle":5}
POST request PROD
URL- https://ehcuspd1-as-ucxaclserviceerss-api.azurewebsites.net/api/eventsource/rehydrate/RequestState/prepare/all
Content-Type: application/json
Settings - {"throttleMs":5,"takePerThrottle":5}
2.	Rehydrate all
o	This has to start after #1 completes.
o	This is also a background task and the endpoint will return an immediate 202 Accepted. 
POST request DEV
URL - https://eheu2dv1-as-ucxaclserviceerss-api.azurewebsites.net/api/eventsource/rehydrate/RequestState/all
Settings - {"throttleMs":5,"takePerThrottle":50,"whereRehydrateRequiredIsTrue":true}
POST request INTG
URL - https://eheu2in1-as-ucxaclserviceerss-api.azurewebsites.net/api/eventsource/rehydrate/RequestState/all
Settings - {"throttleMs":5,"takePerThrottle":50,"whereRehydrateRequiredIsTrue":true}
POST request PROD
URL - https://ehcuspd1-as-ucxaclserviceerss-api.azurewebsites.net/api/eventsource/rehydrate/RequestState/all
Settings - {"throttleMs":5,"takePerThrottle":50,"whereRehydrateRequiredIsTrue":true}

Branching Strategy
Our current approach
We used to follow trunk-based strategy and are still following it with our libraries. But with changes to our release process where we aren’t releasing when ready, we are following this strategy.
Key points
•	main should match what's in PROD.
•	release or hotfix branches will be created with date /yyyy-MM-dd, for what is ready to go to PROD on those dates.
•	feature/task branches will be created from release/hotfix branches.
•	feature/task branches can be deployed to DEV/INTG for functional testing when INTG is not frozen for regression testing.
•	PR to release/hotfix only not to main. Also, PR should be completed only after functional testing and Po sign off is done, to avoid frequent reverts. 
•	only release/hotfix/branches should be deployed in INTG at the time of regression testing.
Possible modifications
•	Have only one release/hotfix branch at a time. i.e. not make a /yyyy-MM-dd
o	Pros
	As these branches are short-lived, the name isn’t important if there is only one release branch most of the time.
	In the case of date changes not updating needed and less confusion on what branch is correct
o	Cons
	For stories which are ready to accept but not planned for release, cannot be PR merged to release branch.

UCX Design Challenges
•	 
Owned by Rajkumar Subaschandra bose
Apr 03, 2024
1 min read
11 people viewed Attachments
 
1.	Kafka Data Storage & Historical Data
a.	Kafka has data since it was launched (Nov 2022), in addition to that, it also has tons of historical data, at least for UCX and being retained forever
b.	Replays & re-hydration take unrealistic time
c.	UCX data is manipulated, there is no confident on data quality
d.	What is the recommendation for
i.	The data retention in Kafka 
ii.	Getting new data elements/schema or historical information to the destination (cosmos) database (via Kafka or outside of Kafka)
iii.	What is the source for historical data? (Qlik from source DB or Data Lake or HI2)
2.	Event Driven Messaging Architecture
a.	It is a good model, but is it really useful for EviCore?
b.	This produces more data, makes coding logic very complex and very expensive
c.	What is the future of this?
3.	UCX Design Review 
a.	UCX follows micro services design pattern, services are too granular currently, there are 25+ micro services. These multiple small services are defined based on UI components
b.	Each UI component / box in a page is defined as services, each service is designed as separate application, each service has its own data consumption and publish logic from and to kafka respectively
i.	Same or similar data is duplicated / replicated across many services
ii.	Very expensive build and code change
c.	Need recommendations to simplify and optimize the services
4.	Cosmos DB Management
a.	Today, dev engineers manage prod Cosmos DBs
b.	Other SQL DBs are managed by IT SE (John Ignatius)
c.	What is the recommendation?
5.	Cloud Strategy
a.	Need design standards and coding recommendations, so things like Azure Cost Optimization could have been avoided all together
6.	Path to Production or Similar
a.	Proactive review and guidance to avoid these reactive work
b.	Tech Stack Review – to avoid individuals preference
c.	Need technology recommendations, review and enterprise standards

Revisit Cache Busting
•	Automated updates to the micro-frontend static JS file URL in UCX.UI_2
Currently the configuration/logic on where the files of a micro-frontend reside is coded within the micro-frontend host. The logic is enhanced to collate the static JS URL with a version which is obtained from another static source.
An implementation exists currently and here are some PRs for the same.
UI_2 enhancement
Front-Door biceps microUI
Front-Door terraform microUI
template changes for json file updating.
microComponents: [
  {
    name: 'ucx-attachments',
    url: 'https://dv1ucxmicroui.blob.core.windows.net/ucxattachmentsmicroui/ucx-attachments.js?sp=r&st=2022-05-10T13:43:52Z&se=2027-05-10T21:43:52Z&spr=https&sv=2020-08-04&sr=b&sig=pZ2T5Rf0bGK0Ur5rDoyW2e39TWqLQPh0zPbiSzYrylM%3D&version=01-11-2024',
    version: 'https://dv1ucxmicroui.blob.core.windows.net/ucxattachmentsmicroui/version.json?sp=r&st=2024-02-20T17:29:34Z&se=2034-02-21T01:29:34Z&spr=https&sv=2022-11-02&sr=b&sig=HFR%2BcgxcagbbcsSMB7UGVCRJ17hz1d1NEVxrFcphMms%3D'
  },
  {
    name: 'ucx-request-provider-details',
    url: 'https://eheu2dv1ucxrpdetailfd.azurefd.net/ucx-request-provider-details.bundle.js?ts=2024-01-10T14:05:53Z',
    version: 'https://eheu2dv1ucxrpdetailfd.azurefd.net/version.json'
  }
];
So, using the following approach, UCX.UI_2 will not have to change if some micro-frondend’s code changes, after the first change to add the version URL.

Discussion Items
lean coffee board.
Completed
1 - Approach to migrate ASE to PEP. a. existing app service to pep b. create a new app with pep (Approach B was recommended by most)
•	Approach A - replace existing plan with new plans.
Cons - Destructive, no testing can be done before switch.
Pros - No changes in existing modules, 
Rollback - Longer
•	Approach B - create new on the side.
PROs - Safer/easier to rollback, some testing can be done before switch, we can also
Cons - Costlier, we would have to think of naming conventions inside the modules, additional release for cleanup.
2 - Thoughts on scaling for replay/rehydration. When does rehydration apply vs replay.
•	Use the default values first, but each domain may have a readme on the most recent runs respective to domains.
•	Discussions have happened on scaling (define and implement a pattern)
•	Discussions on monitoring (need a better progress tracker).
•	Auto scaling
•	If we exceed RUs what is the impact?
3 - App insights data cap, what all do we log to app insights and how much of its is this necessary?
•	Not share app insights across domains, noiser domains.
•	Too many log, debugging and track progress for re-hydration.
•	How was the cap determined?
•	Reduce log level, and making on tracking start and end for a process.
Need to a side discussion on this.

 
Pending Discussions
1 - Any reason why there are times the rehydrates drops Normalized RU consumption per container to below 5-10%
2 - Relook and Rebalance - Product Ownership
In the case of UCX Product Placemat, there were two legacy owners - Gravity and Thunder. Later during Q2 of 2023, Everest and Maverick joined the ownership team, leading to a rebalancing of the ownership structure. Its long time we looked into Ownership.
Additionally, now the focus is on transitioning to a service agnostic approach with the goal of becoming more flexible. The steps involved in this process are as follows:
•	Relook and Rebalance - UCX Product Placemat 
•	Identify Primary and Secondary Ownership 
•	Transition to Service Agnostic
3 - Trunk based development and possibly changing our git branching strategy
4 - We may need good approach for event versioning.

resource "azurerm_frontdoor_rules_engine" "ucx_request_search_rules_engine" {
Add comment116Plus    name                = "CacheBusting"
Add comment117Plus    frontdoor_name      = azurerm_frontdoor.ucx_request_search_front_door.name
Add comment118Plus    resource_group_name = azurerm_frontdoor.ucx_request_search_front_door.resource_group_name
Add comment119Plus  
Add comment120Plus    rule {
Add comment121Plus      name     = "CORS"
Add comment122Plus      priority = 0
Add comment123Plus  
Add comment124Plus      match_condition {
Add comment125Plus        variable = "RequestMethod"
Add comment126Plus        operator = "Equal"
Add comment127Plus        value    = ["GET"]
Add comment128Plus      }
Add comment129Plus  
Add comment130Plus      action {
Add comment131Plus  
Add comment132Plus        response_header {
Add comment133Plus          header_action_type = "Overwrite"
Add comment134Plus          header_name        = "Access-Control-Allow-Origin"
Add comment135Plus          value              = "*"
Add comment136Plus        }
Add comment137Plus      }
Add comment138Plus    }
Add comment139Plus  }

request-search-spa/dist/*.json
Add comment38 TargetFolder: $(Build.ArtifactStagingDirectory)
Add comment39 OverWrite: true
Add comment40Plus   - task: PowerShell@2
Add comment41Plus   displayName: "Generate version file"
Add comment42Plus   inputs:
Add comment43Plus   targetType: "inline"
Add comment44Plus   script: |
Add comment45Plus   $json = '{
Add comment46Plus   "version": $(Build.BuildNumber)
Add comment47Plus   }'
Add comment48Plus   $json | Out-File version.json
Add comment49Plus   workingDirectory: "$(System.ArtifactsDirectory)/request-search-spa/dist/"
Add comment50Plus   pwsh: true # For Linux
Add comment51 - task: PublishBuildArtifacts@1
Add comment52 inputs:
Add comment53 PathtoPublish: $(Build.ArtifactStagingDirectory)

Code Repo	Resource Group	Code Repo	Deployment Pipeline	App Service/Storage Account	Application Insights	Database - Container	Dependencies/Notes	Team most familiar
Activity Log Service	pd1_rsg_cus_ucx	https://dev.azure.com/eviCoreDev/UCX/_git/ActivityLogService
https://dev.azure.com/eviCoreDev/UCX/_build?definitionId=2798
ehcuspd1-asp-activitylogservice	ehcuspd1-ai-ucx	eheu2pd1ucxcosmosdb - RequestForServiceEventsDB	App services and database are in different resource groups.
App services are in resource group pd1_rsg_eu2_ucx-linux	Everest
								



Thanks and Regards
Siraj

From: R, Sirajudeen (CTR) <Sirajudeen.R@evicore.com> 
Sent: Tuesday, August 20, 2024 8:28 PM
To: R, Sirajudeen (CTR) <Sirajudeen.R@evicore.com>
Subject: dsgns prcs



Thanks and Regards
Siraj

